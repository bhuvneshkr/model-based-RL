{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kumar_Bhuvnesh_112689445.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPyidy9z1nj/ZJGXddFl6kA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gA8FHJqiMgOo","colab_type":"text"},"source":["## **OpenAI Gym, PyBullet and PyBulletGym Installation**\n","[Click here to see Gym documentaion](https://gym.openai.com/docs/)\n","\n","[Click here to see PyBullet documentaion](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA)\n","\n","[Click here to see PyBulletGym page](https://github.com/benelot/pybullet-gym)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nipCFbQpMu8h","colab_type":"text"},"source":["**Before we start, first update the apt-get tool in the given machine.**"]},{"cell_type":"code","metadata":{"id":"zViYHb_7C2fi","colab_type":"code","outputId":"3185f0c4-f2cf-4425-f279-18cfbb98f579","executionInfo":{"status":"ok","timestamp":1589239050168,"user_tz":240,"elapsed":6432,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["!apt-get update"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r0% [Waiting for headers] [1 InRelease 2,589 B/88.7 kB 3%] [Connected to cloud.r\r                                                                               \rHit:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","\r0% [Waiting for headers] [1 InRelease 31.5 kB/88.7 kB 36%] [Connected to cloud.\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","\r                                                                               \rGet:5 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n","\r0% [4 InRelease 5,500 B/88.7 kB 6%] [1 InRelease 34.4 kB/88.7 kB 39%] [Connecte\r0% [2 InRelease gpgv 21.3 kB] [4 InRelease 8,396 B/88.7 kB 9%] [1 InRelease 37.\r                                                                               \rGet:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n","\r0% [2 InRelease gpgv 21.3 kB] [4 InRelease 20.0 kB/88.7 kB 23%] [1 InRelease 83\r0% [2 InRelease gpgv 21.3 kB] [4 InRelease 20.0 kB/88.7 kB 23%] [1 InRelease 83\r0% [2 InRelease gpgv 21.3 kB] [4 InRelease 22.9 kB/88.7 kB 26%] [Waiting for he\r                                                                               \r0% [2 InRelease gpgv 21.3 kB] [Waiting for headers] [Waiting for headers]\r                                                                         \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","\r0% [2 InRelease gpgv 21.3 kB] [7 InRelease 9,844 B/74.6 kB 13%] [Waiting for he\r                                                                               \r0% [2 InRelease gpgv 21.3 kB] [Waiting for headers]\r                                                   \rIgn:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [2 InRelease gpgv 21.3 kB] [Waiting for headers]\r                                                   \r0% [Waiting for headers]\r0% [3 InRelease gpgv 242 kB] [Waiting for headers]\r                                                  \rIgn:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\r                                                  \r0% [3 InRelease gpgv 242 kB]\r                            \rHit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,815 kB]\n","Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [876 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [908 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [844 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,205 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,376 kB]\n","Fetched 7,295 kB in 2s (4,269 kB/s)\n","Reading package lists... Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6x2LqNwbMFww","colab_type":"text"},"source":["Most of the requirements of python packages are already fulfilled on Colab. To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes."]},{"cell_type":"code","metadata":{"id":"EY1Z1npZC5fo","colab_type":"code","outputId":"dd99d956-18dd-467c-cb75-2c27632d4951","executionInfo":{"status":"ok","timestamp":1589239064481,"user_tz":240,"elapsed":20719,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":717}},"source":["!pip install gym\n","!apt-get install python-opengl -y\n","!apt install xvfb -y"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.4)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl\n","0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n","Need to get 496 kB of archives.\n","After this operation, 5,416 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Fetched 496 kB in 0s (4,929 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 144429 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n","Need to get 784 kB of archives.\n","After this operation, 2,266 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.4 [784 kB]\n","Fetched 784 kB in 0s (7,896 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 146784 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fH-G48ZaSlFO","colab_type":"text"},"source":["For rendering environment, you can use pyvirtualdisplay. So fulfill that"]},{"cell_type":"code","metadata":{"id":"RD8I53YpC92T","colab_type":"code","outputId":"f336da3e-83ef-4acc-a68e-e995240bdac4","executionInfo":{"status":"ok","timestamp":1589239072823,"user_tz":240,"elapsed":22249,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["!pip install pyvirtualdisplay\n","!pip install piglet"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pyvirtualdisplay\n","  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n","Collecting EasyProcess\n","  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n","Installing collected packages: EasyProcess, pyvirtualdisplay\n","Successfully installed EasyProcess-0.3 pyvirtualdisplay-0.2.5\n","Collecting piglet\n","  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n","Collecting piglet-templates\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/dc/d628dcdf0b38b8f230e9c2309bfa370d2e3fb95e9e9c260213d10fde91ac/piglet_templates-1.0.0-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (19.3.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n","Collecting Parsley\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n","\u001b[K     |████████████████████████████████| 92kB 9.4MB/s \n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.34.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.12.0)\n","Installing collected packages: Parsley, piglet-templates, piglet\n","Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ytIVJtuIaO1b","colab_type":"text"},"source":["**Newly added step: installing Atari!**"]},{"cell_type":"code","metadata":{"id":"1w23_ojAURZL","colab_type":"code","outputId":"cc459ad0-5f6c-4b07-a5ae-c557633afb4e","executionInfo":{"status":"ok","timestamp":1589239075982,"user_tz":240,"elapsed":24434,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!pip install gym[atari]"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.4)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n","Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n","Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n","Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5-z6FT2NYzXE","colab_type":"code","outputId":"c400848f-90b4-47be-ca9a-e49f8c6f73e2","executionInfo":{"status":"ok","timestamp":1589239322804,"user_tz":240,"elapsed":270993,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!pip install pybullet==2.5.9"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting pybullet==2.5.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/8c/e386d9917c3fc8a11523d147d65f19d5211cbfe6ec14adba61ad51e6ac72/pybullet-2.5.9.tar.gz (82.7MB)\n","\u001b[K     |████████████████████████████████| 82.7MB 49kB/s \n","\u001b[?25hBuilding wheels for collected packages: pybullet\n","  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pybullet: filename=pybullet-2.5.9-cp36-cp36m-linux_x86_64.whl size=94375307 sha256=63190bbf1a6a336293694dcdd3ff4ed5b0d881ef09387f594c72ab268e9251e7\n","  Stored in directory: /root/.cache/pip/wheels/e4/be/17/68e1fc91e1594bbc35330ae240f2c74dcce4219786729e034f\n","Successfully built pybullet\n","Installing collected packages: pybullet\n","Successfully installed pybullet-2.5.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HHwqCuAVDrn9","colab_type":"code","outputId":"2f23daaf-aa1a-4b27-ff27-2a24c062015b","executionInfo":{"status":"ok","timestamp":1589239327371,"user_tz":240,"elapsed":275146,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!git clone https://github.com/benelot/pybullet-gym.git"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Cloning into 'pybullet-gym'...\n","remote: Enumerating objects: 735, done.\u001b[K\n","remote: Total 735 (delta 0), reused 0 (delta 0), pack-reused 735\u001b[K\n","Receiving objects: 100% (735/735), 19.29 MiB | 16.66 MiB/s, done.\n","Resolving deltas: 100% (405/405), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_GVojhyza7Nz","colab_type":"code","outputId":"cad60332-46f1-4b3a-8a29-1bd1f93c34a4","executionInfo":{"status":"ok","timestamp":1589239327372,"user_tz":240,"elapsed":274966,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/pybullet-gym/"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/pybullet-gym\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-gEJuj5LDxL0","colab_type":"code","outputId":"c600a498-cb21-4183-fc4f-ef8699453fba","executionInfo":{"status":"ok","timestamp":1589239338329,"user_tz":240,"elapsed":285776,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!pip install -e ."],"execution_count":8,"outputs":[{"output_type":"stream","text":["Obtaining file:///content/pybullet-gym\n","Requirement already satisfied: pybullet>=1.7.8 in /usr/local/lib/python3.6/dist-packages (from pybulletgym==0.1) (2.5.9)\n","Installing collected packages: pybulletgym\n","  Running setup.py develop for pybulletgym\n","Successfully installed pybulletgym\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uZVlna0dTh3N","colab_type":"text"},"source":["Import everything."]},{"cell_type":"code","metadata":{"id":"zuMjm4mjC_9T","colab_type":"code","colab":{}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) # error only\n","import pybulletgym  # register PyBullet enviroments with open ai gym\n","import pybullet\n","import pybullet_data\n","\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n","\n","# Colab comes with PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.autograd as autograd\n","import collections\n","import itertools\n","from collections import deque\n","\n","from torch.autograd import Variable\n","\n","import torchvision.transforms as T\n","from torch.nn import functional as F\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGXHAGHRTJjd","colab_type":"text"},"source":["To activate virtual display, we need to run a script once for training an agent, as follows:"]},{"cell_type":"code","metadata":{"id":"50In5ybcFUMm","colab_type":"code","outputId":"2229d7c1-b61e-4be2-ded2-b14cc40f4525","executionInfo":{"status":"ok","timestamp":1589239342039,"user_tz":240,"elapsed":286839,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"KzcbS-GyTUKG","colab_type":"text"},"source":["The following code creates a virtual display to draw game images on. If you are running locally, just ignore it."]},{"cell_type":"code","metadata":{"id":"5hwrBwc_TSfA","colab_type":"code","colab":{}},"source":["import os\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"310x8KjeDF1n","colab_type":"code","colab":{}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('/content/video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, '/content/video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IqDlu8iZVD3G","colab_type":"text"},"source":["# **Test three environments that we are going to use.**\n","## Breakout\n","\n","Breakout-ram-v0: belongs to  Atari, with discrete action space.\n","\n","See what it looks like: https://gym.openai.com/envs/Breakout-ram-v0/"]},{"cell_type":"code","metadata":{"id":"2pNLys76Wj04","colab_type":"code","outputId":"3b4425e4-fc69-4be9-dcbe-3f90e1b9170b","executionInfo":{"status":"ok","timestamp":1589239351272,"user_tz":240,"elapsed":291723,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":544}},"source":["env = gym.make(\"Breakout-ram-v0\")\n","print(\"env.action_space: \", env.action_space)\n","\n","n_episode = 20    # number of episodes\n","max_steps = 1000   # maximum steps for each episode\n","\n","# Iterate through all episodes\n","for i_episode in range(n_episode):\n","    observation = env.reset()\n","\n","    # Iterate through all steps\n","    for t in range(max_steps):\n","        env.render()\n","        s = observation\n","        a = env.action_space.sample() # sample an action\n","\n","        # One step\n","        observation, r, done, info = env.step(a) # s_prime: next observation, r: immediate reward, done: terminal state indicator\n","        s_prime = observation\n","\n","        # Ends this episode if reaching a terminal state\n","        if done:\n","            print(\"Episode finished after {} timesteps\".format(t+1))\n","            break\n","\n","# should return a state vector if everything worked\n","env.reset()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["env.action_space:  Discrete(4)\n","Episode finished after 164 timesteps\n","Episode finished after 202 timesteps\n","Episode finished after 344 timesteps\n","Episode finished after 197 timesteps\n","Episode finished after 363 timesteps\n","Episode finished after 351 timesteps\n","Episode finished after 314 timesteps\n","Episode finished after 172 timesteps\n","Episode finished after 231 timesteps\n","Episode finished after 181 timesteps\n","Episode finished after 241 timesteps\n","Episode finished after 271 timesteps\n","Episode finished after 209 timesteps\n","Episode finished after 170 timesteps\n","Episode finished after 247 timesteps\n","Episode finished after 281 timesteps\n","Episode finished after 169 timesteps\n","Episode finished after 184 timesteps\n","Episode finished after 318 timesteps\n","Episode finished after 357 timesteps\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([ 63,  63,  63,  63,  63,  63, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n","       255, 255, 255, 255, 192, 192, 192, 192, 192, 192, 255, 255, 255,\n","       255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 240,   0,   0,\n","       255,   0,   0, 240,   0,   5,   0,   0,   6,   0,  70, 182, 134,\n","       198,  22,  38,  54,  70,  88,   6, 146,   0,   8,   0,   0,   0,\n","         0,   0,   0, 241,   0, 242,   0, 242,  25, 241,   5, 242,   0,\n","         0, 255,   0, 228,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0,   0,   8,   0, 255, 255, 255, 255, 255, 255, 255,\n","         0,   0,   5,   0,   0, 186, 214, 117, 246, 219, 242], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"7Ob_jHag5RpB","colab_type":"text"},"source":["# Assignment 3 : Implementation"]},{"cell_type":"code","metadata":{"id":"8gFeihgT5Qyc","colab_type":"code","colab":{}},"source":["class initNeuralNetwork(nn.Module): \n","\n","    def __init__(self, input, output):\n","        super(initNeuralNetwork, self).__init__()\n","        self.input = input\n","        self.output = output\n","\n","        #Neural Network definition (passing inputs and output to train NN\n","        self.neuralNetwork = nn.Sequential(nn.Linear(self.input[0], 128), nn.ReLU(), nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, self.output))\n","\n","    def forward(self, state):\n","        return self.neuralNetwork(state)\n","\n","class BasicBuffer:  #creating buffer to be used as replay buffer for dqn\n","\n","    def __init__(self, max_size):\n","        self.max_size = max_size\n","        self.buffer = collections.deque(maxlen=max_size)\n","        self.buffer = deque(maxlen=max_size)\n","\n","    def push(self, state, action, reward, next_state, done): #Function used to push values in replay buffer\n","        experience = (state.reshape(-1), action, np.array([reward]), next_state.reshape(-1), done)\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        state_batch = []\n","        action_batch = []\n","        reward_batch = []\n","        next_state_batch = []\n","        done_batch = []\n","\n","        batch = random.sample(self.buffer, batch_size) #Generating random samples\n","\n","        for experience in batch: #Batch wise iteration for calculating next step action\n","            state, action, reward, next_state, done = experience\n","            state_batch.append(state)\n","            action_batch.append(action)\n","            reward_batch.append(reward)\n","            next_state_batch.append(next_state)\n","            done_batch.append(done)\n","\n","        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n","      \n","    def __len__(self):  #Calculate length of buffer\n","      return len(self.buffer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zkFC9_vzaEY","colab_type":"code","colab":{}},"source":["env1 = \"Breakout-ram-v0\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Navej2a97PnS","colab_type":"code","colab":{}},"source":["def plot(env_name,episode_rewards): #Function to plot cumulative rewards and episodes\n","  plt.plot(list(range(1,len(episode_rewards)+1)),episode_rewards)\n","  plt.ylabel(\"Cumulative Rewards\")\n","  plt.xlabel(\"Episodes\")\n","  plt.title(env_name)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KolkKraO6oPa","colab_type":"code","colab":{}},"source":["n_episodes = 100\n","max_steps = 1000\n","bining_array = [-1.0, 0.0, 1.0] #bining for discreization\n","batch_size=32\n","target_update_rate = 3"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtmNdKL_BCBg","colab_type":"text"},"source":["# DQN (Assignment 1)"]},{"cell_type":"code","metadata":{"id":"jvWnaL38A_rZ","colab_type":"code","colab":{}},"source":["class DQN_Agent:  #Implementation of DQN\n","\n","    def __init__(self, env, bin_array, learning_rate=3e-4, gamma=0.99,buffer_size=10000):\n","        self.env = env\n","        self.bin_array = bin_array\n","        self.learning_rate = learning_rate\n","        self.gamma = gamma\n","        self.replay_buffer = BasicBuffer(max_size=buffer_size)\n","        self.discretization = {}\n","\n","        self.flag_discrete = not isinstance(env.action_space, gym.spaces.discrete.Discrete)  # Checking for discretization based on the flag\n","        if self.flag_discrete:\n","            num_actions = env.action_space.sample().shape[0]\n","        else:\n","            num_actions = env.action_space.n\n","        tmp1_list = [[*x] for x in list(itertools.product(*[list(range(len(self.bin_array) - 1)) for x in range(num_actions)]))]\n","        for i in range(len(tmp1_list)):\n","            self.discretization[i] = tmp1_list[i]\n","\n","        if isinstance(env.action_space.sample(), int):\n","            self.model = initNeuralNetwork(env.observation_space.shape, env.action_space.n)\n","            self.target_model = initNeuralNetwork(env.observation_space.shape, env.action_space.n)\n","        else:\n","            self.model = initNeuralNetwork(env.observation_space.sample().shape,\n","                             int(math.pow(len(bin_array) - 1, env.action_space.sample().shape[0])))\n","            self.target_model = initNeuralNetwork(env.observation_space.sample().shape,\n","                                    int(math.pow(len(bin_array) - 1, env.action_space.sample().shape[0])))\n","\n","        self.target_model.load_state_dict(self.model.state_dict())\n","        self.target_model.eval()\n","\n","        self.optimizer = torch.optim.Adam(self.model.parameters())\n","        self.L1_loss = nn.L1Loss()\n","\n","    def get_action(self, state, discretize=False, eps=0.20):\n","        state = torch.FloatTensor(state.reshape(-1)).float().unsqueeze(0)\n","        qvals = self.model.forward(state)\n","        action = np.argmax(qvals.cpu().detach().numpy())\n","\n","        if (np.random.randn() < eps):\n","            tmp_action = self.env.action_space.sample()\n","            if discretize:\n","                tmp_action = np.digitize(tmp_action, self.bin_array) - 1\n","\n","            return tmp_action\n","\n","        if discretize:\n","            action = np.array(self.discretization[action])\n","\n","        return action\n","\n","    def compute_loss(self, batch):\n","        states, actions, rewards, next_states, dones = batch\n","        states = torch.FloatTensor(states)\n","        actions = torch.LongTensor(actions)\n","        rewards = torch.FloatTensor(rewards)\n","        next_states = torch.FloatTensor(next_states)\n","        dones = torch.FloatTensor(dones)\n","        curr_Q = self.model.forward(states).gather(1, actions.unsqueeze(1))\n","        curr_Q = curr_Q.squeeze(1)\n","        next_Q = self.target_model.forward(next_states)\n","\n","        max_next_Q = torch.max(next_Q, 1)[0]\n","        expected_Q = rewards.squeeze(1) + self.gamma * max_next_Q\n","\n","        loss = self.L1_loss(curr_Q, expected_Q)\n","        return loss\n","\n","    def update(self, batch_size):\n","        batch = self.replay_buffer.sample(batch_size)\n","        loss = self.compute_loss(batch)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def update_target(self):\n","        self.target_model.load_state_dict(self.model.state_dict())\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3GhWNOZL1id","colab_type":"code","colab":{}},"source":["def train_dqn_learning(env, class_dqn, n_episodes, max_steps, batch_size):\n","    episode_rewards = []\n","    discretize_flag = not isinstance(env.action_space, gym.spaces.discrete.Discrete)\n","    for episode in range(n_episodes):\n","        state = env.reset()\n","        episode_reward = 0\n","        for step in range(max_steps):\n","            action = class_dqn.get_action(state, discretize=discretize_flag)\n","\n","            if discretize_flag:\n","                analog_action = []\n","                for bin in action:\n","                    analog_action.append(np.random.uniform(bining_array[bin],bining_array[bin+1],1))\n","\n","                analog_action = [x[0] for x in analog_action]\n","                next_state, reward, done, _ = env.step(analog_action)\n","                class_dqn.replay_buffer.push(state, action[0], reward, next_state, done)\n","            else:\n","                next_state, reward, done, _ = env.step(action)\n","                class_dqn.replay_buffer.push(state, action, reward, next_state, done)\n","            episode_reward += reward\n","\n","            if len(class_dqn.replay_buffer) > batch_size:\n","                class_dqn.update(batch_size)\n","\n","            if done or step == max_steps-1:\n","                episode_rewards.append(episode_reward)\n","                print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n","                break\n","\n","            state = next_state\n","        \n","        if episode % target_update_rate == 0:\n","            class_dqn.update_target()\n","\n","    return episode_rewards\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbWeqj8BOPZl","colab_type":"code","outputId":"11da1249-e55b-4c7d-b8a3-054b6ebe77e4","executionInfo":{"status":"ok","timestamp":1589218757736,"user_tz":240,"elapsed":113328,"user":{"displayName":"Bhuvnesh Kumar","photoUrl":"","userId":"15446167598137099899"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["env=gym.make(env1)\n","episode_rewards = train_dqn_learning(env, DQN_Agent(env, bining_array), n_episodes, max_steps, batch_size)\n","print(episode_rewards)\n","plot(\"DQN Learning - Breakout-ram-v0\",episode_rewards)\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Episode 0: 0.0\n","Episode 1: 2.0\n","Episode 2: 0.0\n","Episode 3: 2.0\n","Episode 4: 2.0\n","Episode 5: 2.0\n","Episode 6: 0.0\n","Episode 7: 0.0\n","Episode 8: 2.0\n","Episode 9: 0.0\n","Episode 10: 0.0\n","Episode 11: 0.0\n","Episode 12: 2.0\n","Episode 13: 3.0\n","Episode 14: 2.0\n","Episode 15: 2.0\n","Episode 16: 3.0\n","Episode 17: 0.0\n","Episode 18: 0.0\n","Episode 19: 0.0\n","Episode 20: 4.0\n","Episode 21: 2.0\n","Episode 22: 0.0\n","Episode 23: 3.0\n","Episode 24: 9.0\n","Episode 25: 1.0\n","Episode 26: 2.0\n","Episode 27: 0.0\n","Episode 28: 2.0\n","Episode 29: 3.0\n","Episode 30: 0.0\n","Episode 31: 3.0\n","Episode 32: 0.0\n","Episode 33: 3.0\n","Episode 34: 2.0\n","Episode 35: 2.0\n","Episode 36: 2.0\n","Episode 37: 2.0\n","Episode 38: 2.0\n","Episode 39: 2.0\n","Episode 40: 0.0\n","Episode 41: 2.0\n","Episode 42: 4.0\n","Episode 43: 2.0\n","Episode 44: 2.0\n","Episode 45: 2.0\n","Episode 46: 0.0\n","Episode 47: 2.0\n","Episode 48: 2.0\n","Episode 49: 0.0\n","Episode 50: 0.0\n","Episode 51: 0.0\n","Episode 52: 2.0\n","Episode 53: 2.0\n","Episode 54: 2.0\n","Episode 55: 2.0\n","Episode 56: 2.0\n","Episode 57: 2.0\n","Episode 58: 1.0\n","Episode 59: 0.0\n","Episode 60: 2.0\n","Episode 61: 2.0\n","Episode 62: 3.0\n","Episode 63: 2.0\n","Episode 64: 0.0\n","Episode 65: 3.0\n","Episode 66: 2.0\n","Episode 67: 0.0\n","Episode 68: 2.0\n","Episode 69: 2.0\n","Episode 70: 2.0\n","Episode 71: 4.0\n","Episode 72: 0.0\n","Episode 73: 2.0\n","Episode 74: 3.0\n","Episode 75: 4.0\n","Episode 76: 3.0\n","Episode 77: 2.0\n","Episode 78: 3.0\n","Episode 79: 2.0\n","Episode 80: 4.0\n","Episode 81: 2.0\n","Episode 82: 0.0\n","Episode 83: 3.0\n","Episode 84: 1.0\n","Episode 85: 4.0\n","Episode 86: 0.0\n","Episode 87: 0.0\n","Episode 88: 3.0\n","Episode 89: 0.0\n","Episode 90: 0.0\n","Episode 91: 5.0\n","Episode 92: 0.0\n","Episode 93: 0.0\n","Episode 94: 3.0\n","Episode 95: 0.0\n","Episode 96: 0.0\n","Episode 97: 0.0\n","Episode 98: 0.0\n","Episode 99: 3.0\n","[0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 3.0, 2.0, 2.0, 3.0, 0.0, 0.0, 0.0, 4.0, 2.0, 0.0, 3.0, 9.0, 1.0, 2.0, 0.0, 2.0, 3.0, 0.0, 3.0, 0.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 2.0, 2.0, 3.0, 2.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 2.0, 4.0, 0.0, 2.0, 3.0, 4.0, 3.0, 2.0, 3.0, 2.0, 4.0, 2.0, 0.0, 3.0, 1.0, 4.0, 0.0, 0.0, 3.0, 0.0, 0.0, 5.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 3.0]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"NjR2lEjh0nAs","colab_type":"text"},"source":["#DYNA Q with DQN (Image based)"]},{"cell_type":"code","metadata":{"id":"pQOryNKAK6Mz","colab_type":"code","colab":{}},"source":["'''\n","This shows the implementation of the Variation Encoder of the dreaming-policy paper.\n","Variational Autoencoder is used for:\n","   - Encoder: to convert the input image into encoded output images or states\n","   - Decoder: to convert encoded images or states into images output\n","   - Variational AutoEncoder: combines Encoder and Decoder\n","Note: for the following Variationcal encoder layers and CNN definitions are picked similar to the dreaming-policy paper\n","'''\n","\n","'''\n","Encoder Part\n","'''\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, z_dim):\n","        #Initializing the network\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","        nn.Conv2d(3, 16, kernel_size=(4,4), stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(16, 32, kernel_size=(4,4), stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(32, 16, kernel_size=(4,4), stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(16, 32, kernel_size=(4,4), stride=2),\n","        nn.ReLU()\n","        )\n","        # Caluculating mean and variance\n","        self.mean = nn.Conv2d(32, 32, kernel_size=(1,1), stride=1)\n","        self.variance = nn.Conv2d(32, 32, kernel_size=(1,1), stride=1)\n","\n","    def forward(self, x):\n","        #Model to obtain the hidden layer i.e. output fo encoded part\n","        layer = self.encoder(x)\n","        z_mean = self.mean(layer)\n","        z_variance = self.variance(layer)\n","        return z_mean, z_variance\n","\n","'''\n","Decoder Part\n","'''\n","class Decoder(nn.Module):\n","    def __init__(self, z_dim, output_dim, layer_dim=100):\n","        super().__init__()\n","        #Initializing network\n","        self.decoder = nn.Sequential(\n","        nn.Conv2d(32, 64, kernel_size=(3, 3), stride=1, padding=1),\n","        nn.ReLU(),\n","        nn.ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=1),\n","        torch.nn.Upsample(scale_factor=2),\n","        nn.ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=1),\n","        torch.nn.Upsample(scale_factor=2),\n","        nn.ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=1),\n","        torch.nn.Upsample(scale_factor=2),\n","        nn.ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=1),\n","        torch.nn.Upsample(scale_factor=2),\n","        nn.ConvTranspose2d(3, 3, kernel_size=(4, 4), stride=1),\n","        torch.nn.Upsample(size=(64, 64))\n","        )\n","\n","    def forward(self, x):\n","        decoded = self.decoder(x)\n","        return decoded\n","\n","'''\n","Variation Autoencoder\n","'''\n","class VariationalAutoEncoder(nn.Module):\n","    def __init__(self, input_dim, z_dim):\n","        output_dim = input_dim\n","        super(VariationalAutoEncoder, self).__init__()\n","        self.encoder = Encoder(input_dim, z_dim)\n","        self.decoder = Decoder(z_dim, output_dim)\n","\n","    def encode(self, x):\n","        mean,variance = self.encoder(x)\n","        return mean, variance\n","\n","    def decode(self, z):\n","        decoded = self.decoder(z)\n","        return decoded \n","\n","    def reparameterize(self, mean: Variable, log_variance: Variable) -> Variable:\n","        if self.training:\n","            std = log_variance.mul(0.5).exp_()  # type: Variable\n","            eps = Variable(std.data.new(std.size()).normal_())\n","            return eps.mul(std).add_(mean)\n","        else:\n","            return mean\n","            \n","    def forward(self, x):\n","        mean, log_variance = self.encode(x)\n","        z = self.reparameterize(mean, log_variance)\n","        return z, self.decode(z), mean, log_variance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOYp-3x8K6J6","colab_type":"code","colab":{}},"source":["'''\n","This part of the program contains the StateTransitionModel definition\n","which is used to generate the next state based on the present states.\n","For this, input is encoded image (state s) and it generates output encoded images (state s')\n","'''\n","\n","class StateTransitionModel(nn.Module):\n","    def __init__(self, n_action):\n","        super(StateTransitionModel, self).__init__()\n","\n","        self.GenerateAction1 = nn.Sequential(\n","            nn.Linear(n_action, 64),\n","            nn.Linear(64, 128),\n","            nn.LeakyReLU(),\n","        )\n","\n","        self.GenerateAction2 = nn.Sequential(\n","            nn.Conv2d(32,16,kernel_size=(3,3), stride=1, padding=1),\n","            nn.Conv2d(16,32,kernel_size=(1,1),stride=1),\n","            nn.LeakyReLU(),\n","        )\n","\n","        self.regressor = nn.Sequential(\n","        nn.Conv2d(64, 32, kernel_size=(3,3), stride=1, padding=1),\n","        nn.LeakyReLU(),\n","        nn.Conv2d(32, 32, kernel_size=(1,1), stride=1),\n","        nn.LeakyReLU(),\n","        nn.Conv2d(32, 16, kernel_size=(3,3), stride=1, padding=1),\n","        nn.LeakyReLU(),\n","        nn.Conv2d(16, 32, kernel_size=(1,1), stride=1),\n","        nn.LeakyReLU()\n","        )\n","\n","    def forward(self, x, a):\n","        action_generated = torch.reshape(self.GenerateAction1(a), (-1,32,2,2))\n","        action_generated = self.GenerateAction2(action_generated)\n","        x_new = torch.cat((x,action_generated), 1)\n","        return self.regressor(x_new)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxSgIUbSm9G8","colab_type":"code","colab":{}},"source":["'''\n","This class is used to claculate the simulation based reward using the action generated \n","without actual environment\n","'''\n","class RewardObtained(nn.Module):\n","    def __init__(self):\n","        super(RewardObtained, self).__init__()\n","\n","        self.ActionGenerated1 = nn.Sequential(\n","            nn.Conv2d(32, 32, kernel_size=(2,2), stride=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 1, kernel_size=(1,1), stride=2, padding=1)\n","        )\n","        self.ActionGenerated2 = nn.Sequential(\n","            nn.Linear(4, 2),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        values = self.ActionGenerated2(self.ActionGenerated1(x).view(-1,4))\n","        return values[:,0], values[:,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cw7K45VsTV0X","colab_type":"code","colab":{}},"source":["''' \n","This part of the program contains the loss function used for different networks\n","'''\n","\n","#MSE Loss:\n","mse = nn.MSELoss()\n","\n","#Loss for Variation AutoEncoder\n","def VAELossFunction(out, x, mean, variance, batch_size):\n","    BCE = F.binary_cross_entropy_with_logits(out, x, reduce=False)\n","    KLD = -0.5 * torch.sum(1 + variance - mean.pow(2) - variance.exp())\n","    KLD /= batch_size * (output_dim * output_dim)\n","    return (BCE + KLD).mean([1,2,3])\n","\n","#Loss for State Transition Model\n","def RewardLossFunction(actual_reward, predicted_reward, actual_done, predicted_done):\n","    x = actual_reward.reshape(-1)\n","    y = predicted_reward*(1-predicted_done).reshape(-1)\n","    l1 = mse(x,y)\n","    l2=mse(actual_done, predicted_done)\n","    return l1+l2\n","\n","#Loss for Reward calculation model\n","def StateTransitionModelLossFunction(state_trans_inp, enc_next_img, dec_state_trans_inp, next_img):\n","    l1 = F.binary_cross_entropy_with_logits(state_trans_inp, enc_next_img, reduce=False).mean([1,2,3])\n","    l2 = F.binary_cross_entropy_with_logits(dec_state_trans_inp, next_img, reduce=False).mean([1,2,3])\n","    return torch.abs(l1 + l2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zd_qjObDzDeI","colab_type":"code","colab":{}},"source":["'''\n","Function to Generated action based on the input state\n","'''\n","def GenerateAction(state):\n","    sample = random.random()\n","\n","    if (np.random.randn() < epsilon_val):\n","        tmp_action = env.action_space.sample()\n","        return torch.tensor(tmp_action)\n","\n","    with torch.no_grad():\n","        a=policy_net(state)\n","        return a.max(1)[1].view(1, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fui78kBDQrrQ","colab_type":"code","colab":{}},"source":["''' \n","This function is used to render the environment and obtained images of the \n","Breakout environment for different state values\n","'''\n","def get_screen():\n","    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n","    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n","    screen = torch.from_numpy(screen)  \n","    resize = T.Compose([T.ToPILImage(), T.Resize((64,64), interpolation=Image.CUBIC), T.ToTensor()])\n","    screen_val = resize(screen).unsqueeze(0)\n","    return screen_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jb_JfU5Dze4z","colab_type":"code","colab":{}},"source":["'''\n","Updating Policy based on the random state and action pairs\n","'''\n","\n","def PolicyUpdateDQN(learn_steps, target_update_rate):\n","    if learn_steps % target_update_rate:\n","        target_net.load_state_dict(policy_net.state_dict())\n","\n","    #Sample from memory for state and action \n","    sample_states, sample_ar, sample_done = memory.sample(batch_size)\n","    action_data = sample_ar[:,0]\n","    action_data = np.reshape(action_data, (action_data.shape[0], 1))\n","    state_action_values = policy_net(torch.Tensor(sample_states[:,:,0].reshape(-1, 3, 64, 64))).gather(1, torch.tensor(action_data, dtype=torch.long)).reshape(-1)\n","    next_state_action_values = target_net(torch.Tensor(sample_states[:,:,1].reshape(-1, 3, 64, 64)))\n","    target_q_vals = torch.tensor(sample_ar[:,1]) + discount_factor*next_state_action_values.max(1)[0] * torch.tensor(1-sample_done).reshape(-1)\n","\n","    #Calculating loss\n","    dqn_loss = DQN_loss_function(state_action_values, target_q_vals)\n","    dqn_optimizer.zero_grad()\n","    dqn_loss.backward()\n","    dqn_optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mggbTYkiRU3v","colab_type":"code","colab":{}},"source":["'''\n","Updating DYNAQ model based on the random state and action pairs\n","'''\n","\n","def UpdateDYNAQ():\n","    DYNAQ_Model.train()\n","    #Samples of action state pair from memory\n","    sample_states, sample_ar, sample_done = memory.sample(batch_size)\n","    curr_states = torch.Tensor(sample_states[:,:,0].reshape(-1, 3, 64, 64))\n","    action_data = torch.tensor(np.reshape(sample_ar[:, 0], (sample_ar.shape[0], 1)))\n","\n","    reward_data = torch.tensor(sample_ar[:, 1])\n","    state_trans_out, next_pred_img, dec_state_trans_inp, reward_val, done_val, vae_mu, vae_var = DYNAQ_Model(curr_states, action_data, train_flag=True)\n","    next_img = torch.tensor((sample_states[:, :, 1].reshape(-1, 3, 64, 64)))\n","    enc_next_img = DYNAQ_Model.vae.reparameterize(*DYNAQ_Model.vae.encoder(next_img.float()))\n","\n","    #Calculate loss for state, reward values\n","    l1 = RewardLossFunction(reward_data, reward_val, torch.tensor(sample_done), done_val)\n","    l2 = StateTransitionModelLossFunction(state_trans_out, enc_next_img, dec_state_trans_inp, next_img)\n","    l3 = VAELossFunction(dec_state_trans_inp, curr_states, vae_mu, vae_var, batch_size)\n","    dyna_loss = l1+l2+l3\n","    dyna_optimizer.zero_grad()\n","    dyna_loss.sum().backward()\n","    dyna_optimizer.step()\n","    DYNAQ_Model.eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRlO3YxCzbYw","colab_type":"code","colab":{}},"source":["'''\n","Simulating randomly generated state action pair - DYNA Q \n","'''\n","\n","def SimulationDYNA(learn_simulate_steps, target_update_rate):\n","    if learn_simulate_steps % target_update_rate:\n","        target_net.load_state_dict(policy_net.state_dict())\n","\n","    #Samples of state action pair from memory\n","    sample_states, sample_ar, sample_done = memory.sample(1)\n","    curr_states = sample_states[:,:,0].reshape(1,3,64,64)\n","    next_states = sample_states[:, :, 1].reshape(1, 3, 64, 64)\n","    action_data = np.random.randint(0, n_actions-1, curr_states.shape[0])\n","    action_data = np.reshape(action_data, (action_data.shape[0],1))\n","    next_state, reward_val, done_val = DYNAQ_Model(torch.Tensor(curr_states), torch.Tensor(action_data))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUNgtnv7SDZV","colab_type":"code","colab":{}},"source":["'''\n","DQN:\n","This part contains the implementation of Replay Memory and DQN\n","\n","'''\n","\n","class ReplayMemory(object): #Class definition for ReplayMemory\n","\n","    def __init__(self, capacity): #Initializing network\n","        self.capacity = capacity\n","        self.memory_states = np.zeros((capacity, 3*64*64, 2))\n","        self.memory_action_rewards = np.zeros((capacity, 2))\n","        self.memory_done = np.zeros((capacity, 1))\n","        self.position = 0\n","        self.counter = 0\n","\n","    def push(self, params):\n","        # Saves transition\n","        self.memory_states[self.position, :] = np.hstack((params[0].reshape(-1,1), params[2].reshape(-1,1)))\n","        self.memory_action_rewards[self.position, :] = np.hstack((params[1].reshape(-1), params[3]))\n","        self.memory_done[self.position, :] = params[4]\n","        self.position = (self.position + 1) % self.capacity\n","        self.counter = self.counter + 1\n","\n","    def sample(self, batch_size):\n","        indexes = [random.randrange(0, batch_size) for p in range(0, batch_size)]\n","        return np.take(self.memory_states,indexes, axis=0), np.take(self.memory_action_rewards, indexes, axis=0), np.take(self.memory_done, indexes, axis=0)\n","    \n","    def __len__(self):\n","        return self.counter\n","\n","#Class definition of DQN\n","\n","class DQN(nn.Module):\n","    #Initialization of Network\n","    def __init__(self, h, w, outputs):\n","        super(DQN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n","        self.bn1 = nn.BatchNorm2d(16)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n","        self.bn2 = nn.BatchNorm2d(32)\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n","        self.bn3 = nn.BatchNorm2d(64)\n","\n","        # Number of Linear input connections depends on output of conv2d layers\n","        # and therefore the input image size, so compute it.\n","        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n","            return (size - (kernel_size - 1) - 1) // stride  + 1\n","        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n","        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n","        linear_input_size = convw * convh * 64\n","        self.head = nn.Sequential(\n","            nn.Linear(linear_input_size, 64),\n","            nn.Linear(64, outputs)\n","        )\n","\n","    # Called with either one element to determine next action, or a batch\n","    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n","    def forward(self, x):\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        return self.head(x.view(x.size(0), -1))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"atU15yERmO0v","colab_type":"code","colab":{}},"source":["'''\n","DNYAQ:\n","This part contains the implementation of DYNAQ\n","'''\n","#Class definition\n","class DYNAQ(nn.Module):\n","    #Initialization\n","    def __init__(self, input_dim, n_actions):\n","        super().__init__()\n","        n_action_dims = 1\n","        self.vae = VariationalAutoEncoder(input_dim, z_dim)\n","        self.state_transition = StateTransitionModel(n_action_dims)\n","        self.reward_obtained = RewardObtained()\n","\n","    def forward(self, state_image, action, train_flag=False):\n","        encoded_image, decoded_image, mean, variance = self.vae(state_image)\n","        state_transition_output = self.state_transition(encoded_image, action.float())\n","        next_state_img = self.vae.decoder(state_transition_output)\n","        reward, done = self.reward_obtained(state_transition_output)\n","        reward_ = reward * 30\n","        done = torch.where(done>0.5,torch.ones(done.shape),torch.zeros(done.shape))\n","        \n","        if train_flag:\n","            return state_transition_output, next_state_img, decoded_image, reward_, done, mean, variance\n","\n","        return next_state_img, reward_, done\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWYXVA3TauMd","colab_type":"code","colab":{}},"source":["''' \n","This function contains the main training part for the agent\n","In this function for every episode state is being obtained from the screen\n","and based on screen state and action is generated.\n","Now based on this action DYNA Q model (deterministic model) is genrated\n","which is used to update the policy as per the offline implementation\n","'''\n","\n","def CoreImplementationTraining(n_episodes, max_steps, batch_size, target_update_rate): ## Offline DYNA Q Implementation\n","\n","    cumulative_rewards = []\n","    for episode in range(n_episodes):\n","        episode_reward = 0\n","\n","        # Initialize the environment and state (state is obtained from the screen)\n","        env.reset()\n","        state = get_screen()\n","        iter_ = 1\n","        while True:\n","            # Select and perform an action\n","            action = GenerateAction(state)\n","            next_state_, reward, done, _ = env.step(action.item())\n","            \n","            episode_reward += reward\n","            reward = torch.tensor([reward])\n","\n","            current_screen = get_screen()\n","            if not done:\n","                next_state = current_screen\n","            else:\n","                cumulative_rewards.append(episode_reward)\n","                print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n","                break\n","\n","            # Storing the transition in memory\n","            memory.push([state, action, next_state, reward, done])\n","            state = next_state\n","\n","            # Updating Policy\n","            if iter_ % batch_size == 0:\n","                UpdateDYNAQ()\n","                PolicyUpdateDQN(iter_, target_update_rate)\n","\n","            # Simulating for random States and Action Pairs\n","            for iter_2 in range(target_update_rate):\n","                SimulationDYNA(iter_, target_update_rate)\n","\n","\n","            iter_ = iter_ + 1\n","\n","    return cumulative_rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EwzaDa_7QI8n","colab_type":"code","colab":{}},"source":["''' \n","DYNA Q Wrapper Implementation\n","This function acts as an wrapper for all the above function.\n","It creates object for class DQN & DYNAQ and train them respectively\n","Further it updates both the policy and DYNAQ based on the new states obtained.\n","It finally return the episodic reward values\n","'''\n","#Global Params definition\n","\n","epsilon_val = 0.05\n","discount_factor = 0.99\n","target_update_rate = 100\n","batch_size = 50\n","input_dim = 64\n","output_dim = input_dim\n","z_dim = 16\n","\n","env=gym.make(env1)\n","\n","#Generating objects for policy net and target for DQN class\n","n_actions = env.action_space.n\n","policy_net = DQN(input_dim, output_dim, n_actions)\n","target_net = DQN(input_dim, output_dim, n_actions)\n","n_actions = env.action_space.n\n","env.reset()\n","\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","#training for policy & target network\n","policy_net.train()\n","target_net.eval()\n","\n","#initializing Replay memory buffer\n","memory = ReplayMemory(10000)\n","\n","# Updating Policy and DYNA based on loss function\n","DQN_loss_function = nn.MSELoss()\n","DYNAQ_Model = DYNAQ(input_dim, n_actions)\n","DYNAQ_Model.eval()\n","dqn_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n","dyna_optimizer = torch.optim.Adam(DYNAQ_Model.parameters(), lr=0.01)\n","\n","episode_rewards = []\n","\n","#Generating rewards for every episodes\n","dynaq_offline_episode_rewards=episode_rewards.append(CoreImplementationTraining(n_episodes, max_steps, batch_size, target_update_rate))\n","\n","#dynaq_offline_episode_rewards = DYNA_Q_Implementation(env, input_dim, output_dim, z_dim, n_episodes, max_steps, batch_size, target_update_rate)\n","print(dynaq_offline_episode_rewards)\n","plot(\"DYNAQ Learning - Breakout-ram-v0\",dynaq_offline_episode_rewards)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSnI7-OV4Y9u","colab_type":"text"},"source":["## DYNA Q - Online"]},{"cell_type":"code","metadata":{"id":"5VBQMJHN4f3T","colab_type":"code","colab":{}},"source":["''' \n","This function is replica of earlier one except slight change for online deterministic model learning i.e.\n","here instead of updating the policy batch wise i.e. offline we update the policy for every iteration\n","thus here for every episode n number of simulation occurs and model learnt from these is used for every\n","episode to update policy instead of like in prior\n","'''\n","\n","def CoreImplementationTrainingOnline(n_episodes, max_steps, batch_size, target_update_rate): ## Offline DYNA Q Implementation\n","\n","    cumulative_rewards = []\n","    for episode in range(n_episodes):\n","        episode_reward = 0\n","\n","        # Initialize the environment and state (state is obtained from the screen)\n","        env.reset()\n","        state = get_screen()\n","        iter_ = 1\n","        while True:\n","            # Select and perform an action\n","            action = GenerateAction(state)\n","            next_state_, reward, done, _ = env.step(action.item())\n","            \n","            episode_reward += reward\n","            reward = torch.tensor([reward])\n","\n","            current_screen = get_screen()\n","            if not done:\n","                next_state = current_screen\n","            else:\n","                cumulative_rewards.append(episode_reward)\n","                print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n","                break\n","\n","            # Storing the transition in memory\n","            memory.push([state, action, next_state, reward, done])\n","            state = next_state\n","\n","            ### Changes w.r.t. to off line policy\n","            # Updating policy every time instead of batch wise - Online dynamic model learning\n","            # i.e. updating policy and Dyna Q model everytime this Online\n","            \n","            PolicyUpdateDQN(iter_, target_update_rate)\n","            UpdateDYNAQ()\n","                \n","\n","            # Simulating for random States and Action Pairs\n","            for iter_2 in range(target_update_rate):\n","                SimulationDYNA(iter_, target_update_rate)\n","\n","            iter_ = iter_ + 1\n","\n","    return cumulative_rewards"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAfxGyvM4W5Z","colab_type":"code","colab":{}},"source":["''' \n","DYNA Q Wrapper Implementation\n","This function acts as an wrapper for all the above function.\n","It creates object for class DQN & DYNAQ and train them respectively\n","Further it updates both the policy and DYNAQ based on the new states obtained.\n","It finally return the episodic reward values\n","'''\n","#Global Params definition\n","\n","epsilon_val = 0.05\n","discount_factor = 0.99\n","target_update_rate = 100\n","batch_size = 50\n","input_dim = 64\n","output_dim = input_dim\n","z_dim = 16\n","\n","env=gym.make(env1)\n","\n","#Generating objects for policy net and target for DQN class\n","n_actions = env.action_space.n\n","policy_net = DQN(input_dim, output_dim, n_actions)\n","target_net = DQN(input_dim, output_dim, n_actions)\n","n_actions = env.action_space.n\n","env.reset()\n","\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","#training for policy & target network\n","policy_net.train()\n","target_net.eval()\n","\n","#initializing Replay memory buffer\n","memory = ReplayMemory(10000)\n","\n","# Updating Policy and DYNA based on loss function\n","DQN_loss_function = nn.MSELoss()\n","DYNAQ_Model = DYNAQ(input_dim, n_actions)\n","DYNAQ_Model.eval()\n","dqn_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n","dyna_optimizer = torch.optim.Adam(DYNAQ_Model.parameters(), lr=0.01)\n","\n","episode_rewards = []\n","\n","#Generating rewards for every episodes\n","dynaq_online_episode_rewards=episode_rewards.append(CoreImplementationTrainingOnline(n_episodes, max_steps, batch_size, target_update_rate))\n","\n","\n","#dynaq_offline_episode_rewards = DYNA_Q_Implementation(env, input_dim, output_dim, z_dim, n_episodes, max_steps, batch_size, target_update_rate)\n","print(dynaq_online_episode_rewards)\n","plot(\"DYNAQ Learning - Breakout-ram-v0\",dynaq_online_episode_rewards)\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CLv5jEoVIm7","colab_type":"text"},"source":["# Policy Evaluation"]},{"cell_type":"markdown","metadata":{"id":"bgsBQ1rDVLLv","colab_type":"text"},"source":[" Offline"]},{"cell_type":"code","metadata":{"id":"rRvfRugC6Pjt","colab_type":"code","colab":{}},"source":["total_policy_eval_offline=0\n","\n","#Generating objects for policy net and target for DQN class\n","n_actions = env.action_space.n\n","policy_net = DQN(input_dim, output_dim, n_actions)\n","target_net = DQN(input_dim, output_dim, n_actions)\n","n_actions = env.action_space.n\n","env.reset()\n","\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","#training for policy & target network\n","policy_net.train()\n","target_net.eval()\n","\n","#initializing Replay memory buffer\n","memory = ReplayMemory(10000)\n","\n","# Updating Policy and DYNA based on loss function\n","DQN_loss_function = nn.MSELoss()\n","DYNAQ_Model = DYNAQ(input_dim, n_actions)\n","DYNAQ_Model.eval()\n","dqn_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n","dyna_optimizer = torch.optim.Adam(DYNAQ_Model.parameters(), lr=0.01)\n","\n","episode_rewards = []\n","\n","#Generating rewards for every episodes\n","dynaq_offline_episode_rewards=episode_rewards.append(CoreImplementationTraining(10, max_steps, batch_size, target_update_rate))\n","\n","for i in range(10):\n","  total_policy_eval_offline+=dynaq_offline_episode_rewards[i]\n","\n","print(\"Policy Evaluation Offline: \", total_policy_eval_offline/10)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QukUp0QU6e4b","colab_type":"text"},"source":["Online"]},{"cell_type":"code","metadata":{"id":"Y7wS8bJk6d49","colab_type":"code","colab":{}},"source":["\n","total_policy_online_eval=0\n","\n","\n","#Generating objects for policy net and target for DQN class\n","n_actions = env.action_space.n\n","policy_net = DQN(input_dim, output_dim, n_actions)\n","target_net = DQN(input_dim, output_dim, n_actions)\n","n_actions = env.action_space.n\n","env.reset()\n","\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","#training for policy & target network\n","policy_net.train()\n","target_net.eval()\n","\n","#initializing Replay memory buffer\n","memory = ReplayMemory(10000)\n","\n","# Updating Policy and DYNA based on loss function\n","DQN_loss_function = nn.MSELoss()\n","DYNAQ_Model = DYNAQ(input_dim, n_actions)\n","DYNAQ_Model.eval()\n","dqn_optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n","dyna_optimizer = torch.optim.Adam(DYNAQ_Model.parameters(), lr=0.01)\n","\n","episode_rewards = []\n","\n","#Generating rewards for every episodes\n","dynaq_online_episode_rewards=episode_rewards.append(CoreImplementationTrainingOnline(10, max_steps, batch_size, target_update_rate))\n","\n","for i in range(10):\n","  total_policy_online_eval+=dynaq_online_episode_rewards[i]\n","\n","print(\"Policy Evaluation Online: \",total_policy_online_eval/10)"],"execution_count":0,"outputs":[]}]}